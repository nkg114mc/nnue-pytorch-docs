# NNUE

## Preface

What this document DOES contain:

- technical content
- detailed description of NNUE and its principles
- a quick linear algebra refresher
- input definition and factorization
- components (layers) applicable to NNUE networks
- inference code and optimizations
- quantization math and implementation
- almost-production-ready optimized code
- pytorch trainer implementation (+ important CUDA kernels)
- architectural considerations and history

What this document DOES NOT contain:

- a tutorial for training networks (for that see [the wiki](https://github.com/glinscott/nnue-pytorch/wiki))
- datasets, optimizers, hyperparameters
- a log of experimental results

## Table of contents

* [Preface](#preface)
* [Table of contents](#table-of-contents)
* [Basics](#basics)
    + [What is NNUE?](#what-is-nnue)
        - [Quantization 101 and its importance](#quantization-101-and-its-importance)
    + [What layers are useful in NNUE?](#what-layers-are-useful-in-nnue)
        - [Linear layer](#linear-layer)
        - [Linear layer with sparse inputs](#linear-layer-with-sparse-inputs)
        - [Clipped ReLU layer](#clipped-relu-layer)
        - [Sigmoid](#sigmoid)
        - [Quantmoid4](#quantmoid4)
        - [Pooling layers](#pooling-layers)
    + [A simple input feature set.](#a-simple-input-feature-set)
    + [A simple NNUE network](#a-simple-nnue-network)
    + [Consideration of networks size and cost.](#consideration-of-networks-size-and-cost)
        - [Feature set](#feature-set)
        - [First set of hidden neurons](#first-set-of-hidden-neurons)
        - [Further layers](#further-layers)
    + [Accumulator](#accumulator)
    + [HalfKP](#halfkp)
        - [Multiple perspectives, multiple accumulators](#multiple-perspectives--multiple-accumulators)
            * [How to combine multiple accumulator perspectives?](#how-to-combine-multiple-accumulator-perspectives)
            * [Which set of weights to use for each perspective?](#which-set-of-weights-to-use-for-each-perspective)
        - [HalfKP example and network diagram](#halfkp-example-and-network-diagram)
* [Forward pass implementation](#forward-pass-implementation)
    + [Example network](#example-network)
    + [Layer parameters](#layer-parameters)
    + [Accumulator](#accumulator-1)
        - [Refreshing the accumulator](#refreshing-the-accumulator)
        - [Updating the accumulator](#updating-the-accumulator)
    + [Linear layer](#linear-layer-1)
    + [ClippedReLu](#clippedrelu)
    + [Putting it together](#putting-it-together)
* [Training a net with pytorch](#training-a-net-with-pytorch)
    + [Model specification](#model-specification)
    + [Preparing the inputs](#preparing-the-inputs)
        - [Parsing the training data sets and moving them to the python side](#parsing-the-training-data-sets-and-moving-them-to-the-python-side)
        - [Training batch structure and communication](#training-batch-structure-and-communication)
    + [Feature factorization](#feature-factorization)
        - [Virtual feature coalescing](#virtual-feature-coalescing)
        - [Other factors](#other-factors)
            * ["K" factors](#k-factors)
            * ["HalfRelativeKP" factors](#halfrelativekp-factors)
        - [Real effect of the factorizer](#real-effect-of-the-factorizer)
    + [Loss functions and how to apply them](#loss-functions-and-how-to-apply-them)
        - [The Goal](#the-goal)
        - [Converting the evaluation from CP-space to WDL-space](#converting-the-evaluation-from-cp-space-to-wdl-space)
        - [Using results along the evaluation](#using-results-along-the-evaluation)
        - [Mean Squared Error (MSE)](#mean-squared-error-mse)
            * [loss](#loss)
            * [grad](#grad)
        - [Cross entropy](#cross-entropy)
            * [loss](#loss-1)
            * [grad](#grad-1)
* [Quantization](#quantization)
    + [Stockfish quantization scheme](#stockfish-quantization-scheme)
        - [Feature Transformer](#feature-transformer)
        - [Linear layer](#linear-layer-2)
        - [ClippedReLU](#clippedrelu)
    + [The math of quantization and how to make it fit](#the-math-of-quantization-and-how-to-make-it-fit)
        - [Feature Transformer](#feature-transformer-1)
        - [Linear layer](#linear-layer-3)
    + [Implementation](#implementation)
    + [Optimized implementation](#optimized-implementation)
        - [Feature Transformer](#feature-transformer-2)
        - [Linear layer](#linear-layer-4)
            * [m256_add_dpbusd_epi32](#m256_add_dpbusd_epi32)
            * [m256_haddx4](#m256_haddx4)
        - [Linear layer with sparse input](#linear-layer-with-sparse-input)
            * [m256_process_chunk](#m256_process_chunk)
        - [Linear layer with sparse input, alternative approach](#linear-layer-with-sparse-input-alternative-approach)
            * [m256_process_chunk_alternative](#m256_process_chunk_alternative)
        - [Linear layer with sparse input and blocked sparse output](#linear-layer-with-sparse-input-and-blocked-sparse-output)
        - [ClippedReLU](#clippedrelu-1)
            * [int16 -> int8](#int16---int8)
            * [int32 -> int8](#int32---int8)
        - [Quantmoid4](#quantmoid4-1)
        - [Pooling Layers](#pooling-layers)
            * [Average Pooling](#average-pooling)
            * [Max Pooling](#max-pooling)
            * [Product Pooling](#product-pooling)
    + [Accounting for quantization in the trainer](#accounting-for-quantization-in-the-trainer)
        - [Range](#range)
            * [Inside the optimizer](#inside-the-optimizer)
            * [Outside the optimizer](#outside-the-optimizer)
            * [Accounting for virtual layers (factorization)](#accounting-for-virtual-layers-factorization)
        - [Non-differentiable layers](#non-differentiable-layers)
            * [Custom kernel for training-safe amalgamated Quantmoid4](#custom-kernel-for-training-safe-amalgamated-quantmoid4)
* [Optimizing the trainer (CUDA)](#optimizing-the-trainer-cuda)
    + [Using custom CUDA kernels](#using-custom-cuda-kernels)
    + [Feature transformer](#feature-transformer)
        - [New data loader](#new-data-loader)
        - [Feature transformer forward kernel](#feature-transformer-forward-kernel)
        - [Feature transformer backward kernel](#feature-transformer-backward-kernel)
        - [FeatureTransformerSlice layer](#featuretransformerslice-layer)
* [Architectures and new directions](#architectures-and-new-directions)
    + [Simple HalfKP Stockfish architecture](#simple-halfkp-stockfish-architecture)
    + [HalfKAv2 feature set.](#halfkav2-feature-set)
    + [HalfKAv2_hm feature set.](#halfkav2_hm-feature-set)
    + [A part of the feature transformer directly forwarded to the output.](#a-part-of-the-feature-transformer-directly-forwarded-to-the-output)
    + [Multiple PSQT outputs and multiple subnetworks](#multiple-psqt-outputs-and-multiple-subnetworks)
* [Historical Stockfish evaluation network architectures](#historical-stockfish-evaluation-network-architectures)
    + ["SFNNv5" architecture](#sfnnv5-architecture)
    + ["SFNNv4" architecture](#sfnnv4-architecture)
    + ["SFNNv3" architecture](#sfnnv3-architecture)
    + ["SFNNv2" architecture](#sfnnv2-architecture)
    + ["SFNNv1" architecture](#sfnnv1-architecture)
